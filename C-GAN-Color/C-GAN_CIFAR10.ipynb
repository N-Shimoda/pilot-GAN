{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN for CIFAR 10-like images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2369,
     "status": "ok",
     "timestamp": 1629012702418,
     "user": {
      "displayName": "Main Toshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64",
      "userId": "01369137257200460524"
     },
     "user_tz": -540
    },
    "id": "7OhvN34JlwJr",
    "outputId": "48b18d16-d7bb-425c-e6b0-2ea18b8479cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Download and load the CIFAR-100 training & test dataset\n",
    "# root = \"/home/a6000/github/pilot-GAN/C-GAN-Color/cifar100\"\n",
    "root = \"/home/a6000/github/pilot-GAN/C-GAN-Color/cifar10\"\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "        # transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "# train_dataset = datasets.CIFAR100(root, train=True, download=True, transform=data_transforms)\n",
    "train_dataset = datasets.CIFAR10(root, train=True, download=True, transform=data_transforms)\n",
    "# test_dataset = datasets.CIFAR100(root, train=False, download=True, transform=data_transforms)\n",
    "\n",
    "# Combine the training and test datasets\n",
    "# dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# ミニバッチのサイズ\n",
    "batch_size = 50\n",
    "\n",
    "# 訓練データをセットしたデータローダーを作成する\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,  # ミニバッチのサイズは50\n",
    "    shuffle=True,  # データをシャッフルしてから抽出\n",
    ")\n",
    "\n",
    "# 使用可能なデバイスを確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /home/a6000/github/pilot-GAN/C-GAN-Color/cifar10\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
      "           )\n",
      "10 Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "print(train_dataset)\n",
    "print(\"{} Classes: {}\".format(num_classes, train_dataset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design GAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sDIiDtXUnYTV"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"識別器のクラス\n",
    "\n",
    "    Attributes:\n",
    "      layers: Sequentialオブジェクトのリスト\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        \"\"\"\n",
    "        識別器のネットワークを構築する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes: int\n",
    "            Number of classes.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        start_ch = 128  # 先頭層の出力チャネル数\n",
    "        # in_ch = 1 + num_classes  # 入力画像のチャネル数 = 1(グレースケール) + クラス数\n",
    "        in_ch = 3 + num_classes\n",
    "\n",
    "        # 識別器のネットワークを定義する\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 第1層: (bs, 1+num_classes, 28, 28) -> (bs, 128, 14, 14)\n",
    "                nn.Sequential(\n",
    "                    # 畳み込み\n",
    "                    nn.Conv2d(\n",
    "                        in_ch,  # 入力のチャネル数は1\n",
    "                        start_ch,  # フィルター数は128\n",
    "                        4,  # 4×4のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        1,  # 上下左右にサイズ1のパディング\n",
    "                    ),\n",
    "                    # LeakyReLU関数を適用\n",
    "                    # 負の勾配を制御する係数を0.2(デフォルトは0.01)\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                ),\n",
    "                # 第2層: (bs, 128, 14, 14) -> (bs, 256, 7, 7)\n",
    "                nn.Sequential(\n",
    "                    # 畳み込み\n",
    "                    nn.Conv2d(\n",
    "                        start_ch,  # 入力のチャネル数は128\n",
    "                        start_ch * 2,  # フィルター数は128×2\n",
    "                        4,  # 4×4のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        1,  # 上下左右にサイズ1のパディング\n",
    "                    ),\n",
    "                    # 出力値を正規化する(チャネル数は128×2)\n",
    "                    nn.BatchNorm2d(start_ch * 2),\n",
    "                    # LeakyReLU関数を適用\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                ),\n",
    "                # 第3層: (bs, 256, 7, 7) -> (bs, 512, 3, 3)\n",
    "                nn.Sequential(\n",
    "                    # 畳み込み\n",
    "                    nn.Conv2d(\n",
    "                        start_ch * 2,  # 入力のチャネル数は128×2\n",
    "                        start_ch * 4,  # フィルター数は128×4\n",
    "                        3,  # 3×3のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        0,\n",
    "                    ),  # パディングは0(なし)\n",
    "                    # 出力値を正規化する(チャネル数は128×4)\n",
    "                    nn.BatchNorm2d(start_ch * 4),\n",
    "                    # leaky ReLU関数を適用\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                ),\n",
    "                # 第4層: (bs, 512, 3, 3) -> (bs, 1, 1, 1)\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        start_ch * 4,  # 入力のチャネル数は128×4\n",
    "                        1,  # フィルター数は1\n",
    "                        3,  # 3×3のフィルター\n",
    "                        1,  # ストライドは1\n",
    "                        0,\n",
    "                    ),  # パディングは0(なし)\n",
    "                    # 最終出力にはシグモイド関数を適用\n",
    "                    nn.Sigmoid(),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播処理\n",
    "\n",
    "        Parameter:\n",
    "          x: 画像データまたは生成画像\n",
    "        \"\"\"\n",
    "        # 識別器のネットワークに入力して順伝播する\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        # 出力されたテンソルの形状をフラット(bs,)にする\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wxyecq_1nTj1"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"生成器のクラス\n",
    "\n",
    "    Attributes:\n",
    "      layers: Sequentialオブジェクトのリスト\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"生成器のネットワークを構築する\"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        input_dim = 100 + num_classes  # 入力データの次元\n",
    "        out_ch = 128  # 最終層のチャネル数\n",
    "        # img_ch = 1  # 生成画像のチャネル数\n",
    "        img_ch = 3\n",
    "\n",
    "        # 生成器のネットワークを定義する\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 第1層: (bs, 110, 1, 1) -> (bs, 512, 3, 3)\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        input_dim,  # 入力のチャネル数は100\n",
    "                        out_ch * 4,  # フィルター数は128×4\n",
    "                        3,  # 3×3のフィルター\n",
    "                        1,  # ストライドは1\n",
    "                        0,\n",
    "                    ),  # パディングは0(なし)\n",
    "                    # 出力値を正規化する(チャネル数は128×4)\n",
    "                    nn.BatchNorm2d(out_ch * 4),\n",
    "                    # ReLU関数を適用\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                # 第2層: (bs, 512, 3, 3) -> (bs, 256, 7, 7)\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        out_ch * 4,  # 入力のチャネル数は128×4\n",
    "                        out_ch * 2,  # フィルター数は128×2\n",
    "                        3,  # 3×3のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        0,\n",
    "                    ),  # パディングは0(なし)\n",
    "                    # 出力値を正規化する(チャネル数は128×2)\n",
    "                    nn.BatchNorm2d(out_ch * 2),\n",
    "                    # ReLU関数を適用\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                # 第3層: (bs, 256, 7, 7) -> (bs, 128, 14, 14)\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        out_ch * 2,  # 入力のチャネル数は128×2\n",
    "                        out_ch,  # フィルター数は128\n",
    "                        4,  # 4×4のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        1,\n",
    "                    ),  # 上下左右にサイズ1のパディング\n",
    "                    # 出力値を正規化する(チャネル数は128)\n",
    "                    nn.BatchNorm2d(out_ch),\n",
    "                    # ReLU関数を適用\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                # 第4層: (bs, 128, 14, 14) -> (bs, 1, 28, 28)\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        out_ch,  # 入力のチャネル数は128\n",
    "                        img_ch,  # フィルター数は1\n",
    "                        4,  # 4×4のフィルター\n",
    "                        2,  # ストライドは2\n",
    "                        1,\n",
    "                    ),  # 上下左右にサイズ1のパディング\n",
    "                    # Tanh関数を適用\n",
    "                    nn.Tanh(),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"順伝播処理\n",
    "\n",
    "        Parameter:\n",
    "          z: 識別器の出力\n",
    "        \"\"\"\n",
    "        # 生成器のネットワークに入力して順伝播する\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create GAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 重みの初期化を行う関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hgiP3bPGncbM"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    ネットワークの重みを正規分布からサンプリングした値で初期化する\n",
    "\n",
    "    Parameters:\n",
    "      m: ネットワークのインスタンス\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    # 畳み込み層の重み\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)  # 平均0、標準偏差0.02の正規分布\n",
    "        m.bias.data.fill_(0)  # バイアスのみ0で初期化\n",
    "    # バッチ正規化層の重み\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)  # 平均1.0、標準偏差0.02の正規分布\n",
    "        m.bias.data.fill_(0)  # バイアスのみ0で初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 生成器をインスタンス化して重みを初期化する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1627730198814,
     "user": {
      "displayName": "Main Toshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64",
      "userId": "01369137257200460524"
     },
     "user_tz": -540
    },
    "id": "f7VJAquMnf36",
    "outputId": "7aff3deb-6e9d-4dc6-c71a-7d5325c56a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 512, 3, 3]         507,392\n",
      "       BatchNorm2d-2            [-1, 512, 3, 3]           1,024\n",
      "              ReLU-3            [-1, 512, 3, 3]               0\n",
      "   ConvTranspose2d-4            [-1, 256, 7, 7]       1,179,904\n",
      "       BatchNorm2d-5            [-1, 256, 7, 7]             512\n",
      "              ReLU-6            [-1, 256, 7, 7]               0\n",
      "   ConvTranspose2d-7          [-1, 128, 14, 14]         524,416\n",
      "       BatchNorm2d-8          [-1, 128, 14, 14]             256\n",
      "              ReLU-9          [-1, 128, 14, 14]               0\n",
      "  ConvTranspose2d-10            [-1, 3, 28, 28]           6,147\n",
      "             Tanh-11            [-1, 3, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 2,219,651\n",
      "Trainable params: 2,219,651\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 8.47\n",
      "Estimated Total Size (MB): 9.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 生成器をインスタンス化\n",
    "generator = Generator().to(device)\n",
    "# 重みを初期化\n",
    "generator.apply(weights_init)\n",
    "# 生成器のサマリを出力\n",
    "torchsummary.summary(generator, (100 + num_classes, 1, 1))  # 入力テンソルの形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 識別器をインスタンス化して重みを初期化する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1627730198815,
     "user": {
      "displayName": "Main Toshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64",
      "userId": "01369137257200460524"
     },
     "user_tz": -540
    },
    "id": "yhSIPXkqnmNc",
    "outputId": "f4615728-92f1-4602-d3d0-a2db60cd18b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 16, 16]          26,752\n",
      "         LeakyReLU-2          [-1, 128, 16, 16]               0\n",
      "            Conv2d-3            [-1, 256, 8, 8]         524,544\n",
      "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-5            [-1, 256, 8, 8]               0\n",
      "            Conv2d-6            [-1, 512, 3, 3]       1,180,160\n",
      "       BatchNorm2d-7            [-1, 512, 3, 3]           1,024\n",
      "         LeakyReLU-8            [-1, 512, 3, 3]               0\n",
      "            Conv2d-9              [-1, 1, 1, 1]           4,609\n",
      "          Sigmoid-10              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 1,737,601\n",
      "Trainable params: 1,737,601\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.98\n",
      "Params size (MB): 6.63\n",
      "Estimated Total Size (MB): 7.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 識別器をインスタンス化\n",
    "discriminator = Discriminator(num_classes).to(device)\n",
    "# 重みを初期化\n",
    "discriminator.apply(weights_init)\n",
    "# 識別器のサマリを出力\n",
    "# torchsummary.summary(discriminator, (num_classes + 1, 32, 32))  # 入力テンソルの形状\n",
    "torchsummary.summary(discriminator, (num_classes + 3, 32, 32))  # 入力テンソルの形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 損失関数とオプティマイザーの設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0XIwXN38nuaA"
   },
   "outputs": [],
   "source": [
    "# 損失関数はバイナリクロスエントロピー誤差\n",
    "criterion = nn.BCELoss()\n",
    "# 識別器のオプティマイザ−を設定\n",
    "# optimizer_ds = optim.Adam(discriminator.parameters(), lr=0.0003)  # 学習率: デフォルトは0.001\n",
    "optimizer_ds = optim.Adam(discriminator.parameters(), lr=0.001)  # 学習率: デフォルトは0.001\n",
    "# 生成器のオプティマイザーを設定\n",
    "optimizer_gn = optim.Adam(generator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 正解ラベルを One-hot 化する関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q2qf1mp2nx_g"
   },
   "outputs": [],
   "source": [
    "def encoder(label, device, n_class=10):\n",
    "    \"\"\"\n",
    "    正解ラベルをOne-hot表現に変換する\n",
    "    Parameters\n",
    "    ----------\n",
    "    label: 変換対象の正解ラベル\n",
    "    device: 使用するデバイス\n",
    "    n_class: 分類先のクラス数\n",
    "    \"\"\"\n",
    "    # 対角成分の値が1の対角行列を作成\n",
    "    # 2階テンソル(クラスの数, クラスの数)が作成される\n",
    "    one_hot = torch.eye(n_class, device=device)\n",
    "    # ラベルの値のインデックスのOne-hot表現を抽出し、\n",
    "    # 生成器が入力する(バッチサイズ, クラス数, 1, 1)の形状にして返す\n",
    "    return one_hot[label].view(-1, n_class, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 画像のテンソルとラベルのテンソルを結合する関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BzkqmHPrn2nS"
   },
   "outputs": [],
   "source": [
    "def concat_img_label(image, label, device, n_class=10):\n",
    "    \"\"\"\n",
    "    画像のテンソルとラベルのテンソルを連結して識別器に入力するテンソルを作成する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: 画像データを格納したテンソル(bs, 1, 28, 28)\n",
    "    label: 正解ラベル\n",
    "    device: 使用可能なデバイス\n",
    "    n_class: 分類先のクラス数\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    画像とOne-hot化ラベルを結合したテンソル\n",
    "    (bs, 11, 28, 28)\n",
    "    \"\"\"\n",
    "    # 画像が格納されたテンソルの形状を取得する\n",
    "    bs, ch, h, w = image.shape\n",
    "    # ラベルをOne-hot表現に変換\n",
    "    oh_label = encoder(label, device, n_class)\n",
    "    # 画像のサイズに合わせて正解ラベルを(bs, 10, 28, 28)に拡張する\n",
    "    oh_label = oh_label.expand(bs, n_class, h, w)\n",
    "    # 画像(bs, 1(チャネル), 28, 28)とチャネル方向(dim=1)で結合して戻り値とする\n",
    "    return torch.cat((image, oh_label), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 ノイズのテンソルとラベルのテンソルを連結する関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wudp4WDyn6to"
   },
   "outputs": [],
   "source": [
    "def concat_noise_label(noise: torch.Tensor, label: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    ノイズのテンソルとラベルのテンソルを連結して生成器に入力するテンソルを作成する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    noise: torch.Tensor\n",
    "        ノイズのテンソル (bs, 100, 1, 1)\n",
    "    label: int\n",
    "        正解ラベル\n",
    "    device: torch.device\n",
    "        使用するデバイス\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    ノイズとOne-hot化ラベルを連結したテンソル\n",
    "    (bs, 100 + num_classes, 1, 1)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes the size of noise as 100.\n",
    "    \"\"\"\n",
    "    assert noise.shape[1] == 100, \"The size of noise must be 100.\"\n",
    "\n",
    "    # ラベルをOne-hot化\n",
    "    oh_label = encoder(label, device, num_classes)\n",
    "    # ノイズ(bs, 100, 1, 1)とOne-hot化ラベルを\n",
    "    # dim=1で連結して戻り値とする\n",
    "    # print(\"noise: {}\".format(noise.shape))\n",
    "    # print(\"oh_label: {}\".format(oh_label.shape))\n",
    "    return torch.cat((noise, oh_label), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 エポックごとの画像生成に使用するノイズのテンソルを作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8LDhMqVRQlWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
      "        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        8, 9], device='cuda:0')\n",
      "fixed_noise_label.shape: torch.Size([50, 110, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# ノイズの次元数\n",
    "noise_num = 100\n",
    "\n",
    "# 生成器のエポックごとの画像生成に使用するノイズのテンソルを作成\n",
    "# バッチデータと同じ数だけ作成(bs, 100, 1, 1)\n",
    "fixed_noise = torch.randn(\n",
    "    batch_size, noise_num, 1, 1, device=device  # バッチサイズ  # ノイズの次元100  # 1  # 1\n",
    ")\n",
    "# `num_classes` 個の正解ラベル (0 ~ num_classes-1) をバッチデータの数だけ繰り返す\n",
    "# 配列の要素数はバッチデータの数と同じ\n",
    "assert batch_size // num_classes > 0, \"The batch size must be a multiple of the number of classes.\"\n",
    "fixed_label = [i for i in range(num_classes)] * (batch_size // num_classes)\n",
    "print(fixed_label)\n",
    "# ノイズ用の正解ラベルを1階テンソルに変換: (bs,)\n",
    "fixed_label = torch.tensor(fixed_label, dtype=torch.long, device=device)\n",
    "print(fixed_label)\n",
    "# ノイズ(bs, 100, 1, 1)とOne-hot化したラベル (バッチサイズ, クラス数, 1, 1)を\n",
    "# 連結したテンソル (bs, 100+num_classes, 1, 1)を作成\n",
    "fixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device)\n",
    "print(\"fixed_noise_label.shape: {}\".format(fixed_noise_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464295,
     "status": "ok",
     "timestamp": 1627730795888,
     "user": {
      "displayName": "Main Toshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64",
      "userId": "01369137257200460524"
     },
     "user_tz": -540
    },
    "id": "UAYdTff3oCGZ",
    "outputId": "61337de4-9074-4774-f05e-05a21012934a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 13, 4, 4], expected input[50, 11, 28, 28] to have 13 channels, but got 11 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:60\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 93\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 識別器のネットワークに入力して順伝播する\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 出力されたテンソルの形状をフラット(bs,)にする\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 13, 4, 4], expected input[50, 11, 28, 28] to have 13 channels, but got 11 channels instead"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epoch = 20  # 学習回数\n",
    "\n",
    "# 画像の保存先のパス\n",
    "# パスは環境に合わせて書き換えることが必要\n",
    "# outf = '/content/drive/MyDrive/Colab Notebooks/C-GAN/C-GAN_PyTorch/result'\n",
    "# outf = \"result\"\n",
    "outf = \"/home/a6000/github/pilot-GAN/C-GAN-Color/result\"\n",
    "\n",
    "# save initial image (should be random noise)\n",
    "fake_image = generator(fixed_noise_label)\n",
    "vutils.save_image(\n",
    "    fake_image.detach(),\n",
    "    \"{}/fake_samples_epoch_{:03d}.png\".format(outf, 0),\n",
    "    normalize=True,\n",
    "    nrow=10,\n",
    ")\n",
    "\n",
    "# 学習のループ\n",
    "for epoch in range(n_epoch):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, n_epoch))\n",
    "\n",
    "    # バッチデータのループ(ステップ)\n",
    "    for itr, data in enumerate(dataloader):\n",
    "        # ミニバッチのすべての画像を取得\n",
    "        real_image = data[0].to(device)\n",
    "        # ミニバッチのすべての正解ラベルを取得\n",
    "        real_label = data[1].to(device)\n",
    "        # 画像とOne-hot化したラベルを連結したテンソルを取得\n",
    "        # (bs, 1+num_classes, 28, 28)\n",
    "        real_image_label = concat_img_label(\n",
    "            real_image, real_label, n_class=num_classes, device=device\n",
    "        )  # 画像  # 正解ラベル\n",
    "        # 標準正規分布からノイズを生成: 出力(bs, 100, 1, 1)\n",
    "        noise = torch.randn(\n",
    "            batch_size, noise_num, 1, 1, device=device  # バッチサイズ  # ノイズの次元100  # 1  # 1\n",
    "        )\n",
    "        # フェイク画像用の正解ラベルを生成: 出力(bs,)\n",
    "        fake_label = torch.randint(\n",
    "            10,  # 0～9のラベルを生成\n",
    "            (batch_size,),  # バッチデータの数だけ生成\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "        # ノイズとフェイクのラベルを連結: (bs, 110, 1, 1)\n",
    "        fake_noise_label = concat_noise_label(\n",
    "            noise, fake_label, device  # (bs, 100, 1, 1)  # (bs,)\n",
    "        )\n",
    "        # オリジナル画像に対する識別信号の正解値「1」で初期化した\n",
    "        # (bs,)の形状のテンソルを生成\n",
    "        real_target = torch.full((batch_size,), 1.0, device=device)\n",
    "        # 生成画像に対する識別信号の正解値「0」で初期化した\n",
    "        # (bs,)の形状のテンソルを生成\n",
    "        fake_target = torch.full((batch_size,), 0.0, device=device)\n",
    "\n",
    "        # -----識別器の学習-----\n",
    "        # 識別器の誤差の勾配を初期化\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # 識別器に画像とラベルのセットを入力して識別信号を出力\n",
    "        output = discriminator(real_image_label)\n",
    "        # オリジナル画像に対する識別値の損失を取得\n",
    "        ds_real_err = criterion(output, real_target)\n",
    "        # 1ステップ(1バッチ)におけるオリジナル画像の識別信号の平均\n",
    "        true_dsout_mean = output.mean().item()\n",
    "\n",
    "        # ノイズとフェイクのラベルを生成器に入力してフェイク画像を生成\n",
    "        # (bs, 1, 28, 28)\n",
    "        fake_image = generator(fake_noise_label)  # (bs, 110, 1, 1)\n",
    "        # フェイク画像とフェイクのラベルを連結: (bs, 11, 28, 28)\n",
    "        fake_image_label = concat_img_label(\n",
    "            fake_image, fake_label, n_class=num_classes, device=device  # (bs, 1, 28, 28)  # (bs,)\n",
    "        )\n",
    "\n",
    "        # フェイク画像とフェイクラベルを識別器に入力して識別信号を出力\n",
    "        output = discriminator(fake_image_label.detach())\n",
    "        # フェイク画像を偽と判定できない場合の損失\n",
    "        ds_fake_err = criterion(\n",
    "            output, fake_target  # フェイク画像の識別信号\n",
    "        )  # 正解ラベル(偽物の0)\n",
    "        # フェイク画像の識別信号の平均\n",
    "        fake_dsout_mean1 = output.mean().item()\n",
    "        # オリジナル画像とフェイク画像に対する識別の損失を合計して\n",
    "        # 識別器としての損失を求める\n",
    "        ds_err = ds_real_err + ds_fake_err\n",
    "\n",
    "        # 識別器全体の誤差を逆伝播\n",
    "        ds_err.backward()\n",
    "        # 判別器の重みのみを更新(生成器は更新しない)\n",
    "        optimizer_ds.step()\n",
    "\n",
    "        # -----生成器の学習-----\n",
    "        # 生成器の誤差の勾配を初期化\n",
    "        generator.zero_grad()\n",
    "        # 更新後の識別器に再度フェイク画像とフェイクラベルを入力して識別信号を取得\n",
    "        output = discriminator(fake_image_label)\n",
    "        # フェイク画像をオリジナル画像と誤認できない場合の損失\n",
    "        gn_err = criterion(\n",
    "            output, real_target  # フェイク画像の識別信号\n",
    "        )  # 誤認させるのが目的なので正解ラベルは1\n",
    "        # 更新後の識別器の誤差を逆伝播\n",
    "        gn_err.backward()\n",
    "        # 更新後の識別器のフェイク画像に対する識別信号の平均\n",
    "        fake_dsout_mean2 = output.mean().item()\n",
    "        # 生成器の重みを更新後の識別誤差の勾配で更新\n",
    "        optimizer_gn.step()\n",
    "\n",
    "        # 100ステップごとに出力\n",
    "        if itr % 100 == 0:\n",
    "            print(\n",
    "                \"({}/{}) ds_loss: {:.3f} - gn_loss: {:.3f} - true_out: {:.3f} - fake_out: {:.3f}>>{:.3f}\".format(\n",
    "                    itr + 1,  # ステップ数(イテレート回数)\n",
    "                    len(dataloader),  # ステップ数(1エポックのバッチ数)\n",
    "                    ds_err.item(),  # 識別器の損失\n",
    "                    gn_err.item(),  # フェイクをオリジナルと誤認しない損失\n",
    "                    true_dsout_mean,  # オリジナル画像の識別信号の平均\n",
    "                    fake_dsout_mean1,  # フェイク画像の識別信号の平均\n",
    "                    fake_dsout_mean2,\n",
    "                )  # 更新後識別器のフェイクの識別信号平均\n",
    "            )\n",
    "\n",
    "        # 学習開始直後にオリジナル画像を保存する\n",
    "        if epoch == 0 and itr == 0:\n",
    "            vutils.save_image(\n",
    "                real_image, \"{}/real_samples.png\".format(outf), normalize=True, nrow=10\n",
    "            )\n",
    "\n",
    "    # 確認用画像の生成\n",
    "    # 1エポック終了ごとに確認用の生成画像を生成する\n",
    "    fake_image = generator(fixed_noise_label)\n",
    "    vutils.save_image(\n",
    "        fake_image.detach(),\n",
    "        \"{}/fake_samples_epoch_{:03d}.png\".format(outf, epoch + 1),\n",
    "        normalize=True,\n",
    "        nrow=10,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPoE4FLY/QwY2OJzNGYCtES",
   "collapsed_sections": [],
   "mount_file_id": "1yyNwZ-FkZlnt0F5_clgAxZb33ujtaxV9",
   "name": "C-GAN_MNIST_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
